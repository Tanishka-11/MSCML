{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d4c358",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393fbf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T10:24:30.622397Z",
     "start_time": "2024-06-06T10:24:28.271543Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219056c",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f242dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T10:24:53.700424Z",
     "start_time": "2024-06-06T10:24:53.660345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add a bias term (column of ones) to the data\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ad8e7",
   "metadata": {},
   "source": [
    "#### Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3dee2",
   "metadata": {},
   "source": [
    "Purpose: The softmax function is used to convert raw scores (logits) into probabilities. It's often used in classification problems to represent the probability distribution over different classes.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Stabilize Computation: Subtract the maximum value in each row of logits to prevent overflow when computing the exponentials. This is a numerical stability trick.\n",
    "\n",
    "Exponentiate: Compute the exponentials of the stabilized logits.\n",
    "\n",
    "Normalize: Divide by the sum of exponentials for each row to get probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d1564",
   "metadata": {},
   "source": [
    "Purpose: This function calculates the loss (negative log-likelihood) and gradients for the softmax regression model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute Logits: Calculate the raw scores by multiplying the feature matrix X by the parameter matrix theta.\n",
    "\n",
    "Compute Probabilities: Apply the softmax function to the logits to get the predicted probabilities for each class.\n",
    "\n",
    "Compute Loss:\n",
    "m is the number of training examples.\n",
    "\n",
    "np.arange(m) generates an array [0, 1, 2, ..., m-1], which represents the indices of training examples.\n",
    "\n",
    "y_proba[np.arange(m), y] selects the predicted probabilities corresponding to the true class labels.\n",
    "\n",
    "np.log(y_proba[np.arange(m), y]) computes the logarithm of these probabilities.\n",
    "\n",
    "-np.mean(np.log(y_proba[np.arange(m), y])) computes the mean negative log-likelihood loss.\n",
    "\n",
    "Compute Gradients:\n",
    "\n",
    "np.eye(np.max(y) + 1)[y] creates a one-hot encoded matrix of the true class labels.\n",
    "\n",
    "y_proba - np.eye(np.max(y) + 1)[y] computes the difference between the predicted probabilities and the one-hot encoded true labels.\n",
    "\n",
    "X.T.dot(y_proba - np.eye(np.max(y) + 1)[y]) computes the gradient of the loss with respect to theta.\n",
    "\n",
    "The gradients are averaged over all training examples by dividing by m.\n",
    "\n",
    "Return Loss and Gradients: The function returns the computed loss and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc8ea1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Purpose: This function is used to make predictions using the softmax regression model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute Logits: Calculate the raw scores by multiplying the feature matrix X by the parameter matrix theta.\n",
    "\n",
    "Compute Probabilities: Apply the softmax function to the logits to get the predicted probabilities for each class.\n",
    "\n",
    "Make Predictions: Use np.argmax to find the class with the highest probability for each example. This function returns the indices of the maximum values along the specified axis (in this case, axis 1, which corresponds to classes).\n",
    "\n",
    "Summary\n",
    "\n",
    "The softmax function converts raw scores to probabilities.\n",
    "\n",
    "The compute_loss_and_gradients function calculates the negative log-likelihood loss and gradients for the softmax regression model.\n",
    "\n",
    "The predict function makes predictions by finding the class with the highest predicted probability for each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1699cd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T10:25:18.792106Z",
     "start_time": "2024-06-06T10:25:18.777140Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def compute_loss_and_gradients(X, y, theta):\n",
    "    logits = X.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    m = X.shape[0]\n",
    "    entropy_loss = -np.mean(np.log(y_proba[np.arange(m), y]))\n",
    "    gradients = (1/m) * X.T.dot(y_proba - np.eye(np.max(y) + 1)[y])\n",
    "    return entropy_loss, gradients\n",
    "\n",
    "def predict(X, theta):\n",
    "    logits = X.dot(theta)\n",
    "    return np.argmax(softmax(logits), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf73cb",
   "metadata": {},
   "source": [
    "#### Implement Batch Gradient Descent with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e9286",
   "metadata": {},
   "source": [
    "Function Purpose\n",
    "The softmax_regression function aims to train a softmax regression model on the training data while monitoring the performance on the validation data. It uses batch gradient descent to minimize the loss function and incorporates early stopping to prevent overfitting.\n",
    "\n",
    "Key Components\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Input Dimensions: Determine the number of input features (n_inputs) and the number of output classes (n_outputs).\n",
    "\n",
    "Parameter Initialization: Initialize the parameter matrix theta with random values.\n",
    "\n",
    "Early Stopping Parameters: Initialize best_loss to infinity and epochs_without_improvement to zero. These will be used to track the best validation loss and the number of epochs without improvement, respectively.\n",
    "\n",
    "Training Loop:\n",
    "\n",
    "The loop runs for a maximum of n_epochs iterations.\n",
    "\n",
    "Compute Loss and Gradients: Calculate the training loss and gradients using the compute_loss_and_gradients function.\n",
    "\n",
    "Update Parameters: Update the parameter matrix theta using the gradient descent update rule.\n",
    "\n",
    "Validation Loss: Calculate the loss on the validation set to monitor performance.\n",
    "\n",
    "Early Stopping: Check if the validation loss has improved. If it has, update best_loss and reset epochs_without_improvement. If not, increment epochs_without_improvement.\n",
    "\n",
    "Stopping Condition: If the model has not improved for patience consecutive epochs, stop training early to prevent overfitting.\n",
    "\n",
    "Summary\n",
    "Initialization: Set up dimensions, initialize parameters, and early stopping variables.\n",
    "\n",
    "Training Loop: For each epoch:\n",
    "Calculate loss and gradients on the training set.\n",
    "\n",
    "Update parameters using gradient descent.\n",
    "\n",
    "Calculate loss on the validation set.\n",
    "\n",
    "Check for early stopping conditions:\n",
    "If validation loss improves, update the best loss and reset the counter.\n",
    "If validation loss does not improve for a specified number of epochs (patience), stop training early.\n",
    "\n",
    "Return: The learned parameters (theta) after training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e7706f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T10:25:49.506139Z",
     "start_time": "2024-06-06T10:25:49.145763Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_regression(X_train, y_train, X_val, y_val, learning_rate=0.01, n_epochs=1000, tol=1e-4, patience=5):\n",
    "    n_inputs = X_train.shape[1]\n",
    "    n_outputs = np.max(y_train) + 1\n",
    "    theta = np.random.randn(n_inputs, n_outputs)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss, gradients = compute_loss_and_gradients(X_train, y_train, theta)\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        val_loss, _ = compute_loss_and_gradients(X_val, y_val, theta)\n",
    "        \n",
    "        if val_loss < best_loss - tol:\n",
    "            best_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "    return theta\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "theta = softmax_regression(X_train_split, y_train_split, X_val_split, y_val_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb46f03",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee42240c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T10:26:24.742398Z",
     "start_time": "2024-06-06T10:26:24.721492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on the test set\n",
    "y_pred = predict(X_test, theta)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
